{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:tf_m1] *",
      "language": "python",
      "name": "conda-env-tf_m1-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "name": "lab-03-linpeiZhang.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c99f4ed-d6e8-4f0c-bdd4-bf6c1662a39f"
      },
      "source": [
        "# Lab 03: Text Classification on the DBpedia14 dataset"
      ],
      "id": "5c99f4ed-d6e8-4f0c-bdd4-bf6c1662a39f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92e74017-06ac-4e30-afe2-438f413d2a95"
      },
      "source": [
        "### Objectives:\n",
        "1. Build a Naive Bayes classification model from scratch\n",
        "2. Evaluate the performance of your model on the DBpedia14 dataset\n",
        "3. Train an off-the-shelf NB classifier and compare its performance to your implementation\n",
        "4. Train off-the-shelf implementations of the linear-SVM, RBF-kernel-SVM, and perceptron and compare their performance with the NB models"
      ],
      "id": "92e74017-06ac-4e30-afe2-438f413d2a95"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65ad5be2-5882-4cb6-95cb-18641199b02b"
      },
      "source": [
        "### Suggested Reading\n",
        "\n",
        "1. https://arxiv.org/pdf/1811.12808.pdf"
      ],
      "id": "65ad5be2-5882-4cb6-95cb-18641199b02b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6db7b42-9a27-45bd-b0e4-77106357c9d6"
      },
      "source": [
        "### Download the dataset"
      ],
      "id": "f6db7b42-9a27-45bd-b0e4-77106357c9d6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a83cd2ac-a840-4d36-b22d-3b4215cb99d6",
        "outputId": "55670be7-b1d5-4d3e-8267-25459c74effd"
      },
      "source": [
        "import datasets\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "\n",
        "#train_ds, test_ds = datasets.load_dataset('dbpedia_14', split=['train[:80%]', 'test[80%:]'])\n",
        "#df_train: pd.DataFrame = train_ds.to_pandas()\n",
        "#df_test: pd.DataFrame = test_ds.to_pandas()\n",
        "\n",
        "\n",
        "ds = datasets.load_dataset('dbpedia_14', split='train')\n",
        "df_ds: pd.DataFrame = ds.to_pandas()\n",
        "df_ds= df_ds.sample(frac=0.1)\n",
        "X,Y = df_ds['content'], df_ds['label']\n",
        "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y, test_size=0.2, random_state=123, stratify=Y)"
      ],
      "id": "a83cd2ac-a840-4d36-b22d-3b4215cb99d6",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset d_bpedia14 (/root/.cache/huggingface/datasets/d_bpedia14/dbpedia_14/2.0.0/7f0577ea0f4397b6b89bfe5c5f2c6b1b420990a1fc5e8538c7ab4ec40e46fa3e)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "9a791197-515c-4dce-aac8-e8a4ba40be10"
      },
      "source": [
        "# Part I: Build your own Naive Bayes classification model"
      ],
      "id": "9a791197-515c-4dce-aac8-e8a4ba40be10"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eb2542f-9461-427d-93bb-1d39b1627afe"
      },
      "source": [
        "### (5 pts) Task I: Build a model from scratch\n",
        "Using your notes from lecture-02, implement a Naive Bayes model and train it on the DBpedia dataset. Also, feel free to use any text preprocessing you wish, such as the pipeline from Lab02. \n",
        "\n",
        "Below is a template class to help you think about the structure of this problem (feel free to design your own code if you like). It contains methods for each inference step in NB. It also has a classmethod that you could use to instantiate the class from a list of documents and a corresponding list of labels. Here we are suggesting you create a dictionary that maps each word to a unique $ith$ index in the $\\phi_{i,k}$ probabilty matrix, which you need to estimate. Because the labels are a set of 0-indexed integers, they naturally map to a unique position $\\mu_{k}$ (you should check this to make sure)."
      ],
      "id": "0eb2542f-9461-427d-93bb-1d39b1627afe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a7fae75-aed4-46d4-8bca-b31fe3ce3654"
      },
      "source": [
        "from typing import Union, List\n",
        "import numpy as np\n",
        "\n",
        "Y_train=np.array(list(Y_train))\n",
        "\n",
        "class NaiveBayesModel:\n",
        "    \n",
        "    \"\"\"Multinomial NB model class template\"\"\"\n",
        "    \n",
        "    phi: np.ndarray # (N, K)\n",
        "    \n",
        "    mu: np.ndarray  # (K,)\n",
        "    \n",
        "    vocab: dict     # vocabulary map from word to row index in phi\n",
        "    \n",
        "    n_class: int    # number of classes\n",
        "    \n",
        "    \n",
        "    def __init__():\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        vocabulary: {str: int} <- {word: index}\n",
        "        num_classes: Number of classes\n",
        "        \"\"\"\n",
        "        docs_list= list(X_train)\n",
        "        \n",
        "        vocabulary = {word: idx for idx, word in enumerate(set(\" \".join(docs_list).split(\" \")))}\n",
        "        \n",
        "        num_classes= len(set(Y_train))\n",
        "\n",
        "    \n",
        "        mu_hat = np.array([sum(Y_train == idx) / len(docs_list) for idx in range(num_classes)])\n",
        "        \n",
        "        \n",
        "        \n",
        "        Xtr = np.zeros(shape=(len(docs_list), len(vocabulary)))\n",
        "        for i, doc in enumerate(docs_list):\n",
        "            for word in doc.split(\" \"):\n",
        "                j = vocabulary[word]\n",
        "                Xtr[i, j] += 1\n",
        "        \n",
        "        word_count_by_class = {k: np.sum(Xtr[np.where(Y_train == k)]) for k in range(num_classes)}\n",
        "\n",
        "        \n",
        "        \n",
        "        alpha = 1.0\n",
        "        phi_hat = np.zeros(shape=(num_classes, len(vocabulary)))\n",
        "        for word, j in vocabulary.items():\n",
        "            for k in range(num_classes):\n",
        "                num_word_j_class_k = sum(np.squeeze(Xtr[np.where(Y_train == k), j]))\n",
        "                phi_hat[k, j] = (alpha + num_word_j_class_k) / (alpha * len( vocabulary) + word_count_by_class[k])\n",
        "        np.sum(phi_hat, axis=1)\n",
        "        \n",
        "    \n",
        "        p_y_given_Xte=[]\n",
        "        yte_hat=[]\n",
        "        Xte = np.zeros(shape=(len(X_test), len(vocabulary)))\n",
        "        for i, doc in enumerate(list(X_test)):\n",
        "            for word in doc.split(\" \"):\n",
        "                if word in vocabulary:\n",
        "                    j = vocabulary[word]\n",
        "                    Xte[i,j] += 1\n",
        "                else:\n",
        "                    pass\n",
        "        return Xte,phi_hat,mu_hat"
      ],
      "id": "0a7fae75-aed4-46d4-8bca-b31fe3ce3654",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc4fc2ba-1443-44e4-b6cd-42fef40c677b"
      },
      "source": [
        "Xte,phi_hat,mu_hat= NaiveBayesModel.__init__()"
      ],
      "id": "dc4fc2ba-1443-44e4-b6cd-42fef40c677b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d34cbc5a"
      },
      "source": [
        "yte_hat=np.zeros(len(Xte))\n",
        "p_y=[]\n",
        "for i in range(len(Xte)):\n",
        "            p_y_given_Xte = Xte[i].dot(np.log(phi_hat).T) + np.log(mu_hat)\n",
        "            p_y.append(list(p_y_given_Xte))\n",
        "            yte_hat[i] = np.argmax(p_y_given_Xte)"
      ],
      "id": "d34cbc5a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d193bce-b931-4cd6-b853-7bfb951bbcc5"
      },
      "source": [
        "# Part II: Model performance evaluation\n",
        "\n",
        "Evaluating the performance of a classification model may seem as simple as computing an accuracy, and in some cases that is sufficient, but in general accuracy is not a reliable metric by itself. Typically we need to evaluate our model using several different metrics. \n",
        "\n",
        "One common issue is class imbalance, which is when the label distribution in the data varies far from uniform. In this case a high accuracy can be misleading because low frequency labels don't contribute equally to the score. More generally, this is one of the biggest drawbacks of using MLE in NLP: models tend to be much less sensitive to low probability labels than to higher probabilty labels. Later in this class we will explore models that learn by predicting words given their context, can you think of reasons why this can be problematic? Hint: remember Zipf's law?\n",
        "\n",
        "Another reason to use multiple evaluation methods is that it can help you better understand your data. Evaluating performance on individual classes often reveals problems with the data that would otherwise go unnoticed. For example, if you observe an abundance of misclassified data specific to only a few classes, chances are you have inconsistent labels for those classes in the training set. This is very common in 3rd party mechanical turk data, where quality can vary wildly.\n",
        "\n",
        "In this lab we will use three metrics and one visualization tool:\n",
        "\n",
        "1. [Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision)\n",
        "2. [F1 score](https://en.wikipedia.org/wiki/F-score)\n",
        "3. [AUC ROC score](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
        "4. [The confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
        "\n",
        "The [metrics module](https://scikit-learn.org/stable/modules/model_evaluation.html) within sklearn provides support for nearly any evaluation metric that you will need."
      ],
      "id": "6d193bce-b931-4cd6-b853-7bfb951bbcc5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8801d31f-1131-476e-976d-02800bf01e56"
      },
      "source": [
        "# Part III: Compare your performance to an off-the-shelf NB classifier\n",
        "Open source implementations of your custom NB classifier from Part I already exist of course. One such implementation is [`sklearn.naive_bayes.MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) from the sklearn library. \n",
        "\n",
        "### (5 pts) Task II: NB model comparison\n",
        "Train this model on the same data and compare its performance with your model using the metrics from part II."
      ],
      "id": "8801d31f-1131-476e-976d-02800bf01e56"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c4e3f58-b1f7-471b-b3d0-35d1a55f6782"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import sklearn.metrics\n",
        "params = {'alpha': [0.5, 0.7,1,2],\n",
        "         }\n",
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf',GridSearchCV(MultinomialNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5)),\n",
        "])\n",
        "# Fitting our train data to the pipeline\n",
        "text_clf.fit(X_train, Y_train)\n",
        "# Predicting our test data\n",
        "predicted = text_clf.predict(X_test)\n",
        "print(classification_report(Y_test,predicted ))"
      ],
      "id": "2c4e3f58-b1f7-471b-b3d0-35d1a55f6782",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f21bc2a8",
        "outputId": "7deef164-ac10-42ee-f5ed-02d094c6f0f1"
      },
      "source": [
        "results= np.reshape([nb_accuracy,mm_accuracy,nb_f1score,mm_f1score,nb_AUC,mm_AUC],(3,2))\n",
        "measurement= pd.DataFrame(data=results, index=[\"Accuracy\",\"F1 score\",\"AUC ROC score\"], columns=[\"NB model\",\"My model\"])\n",
        "print(measurement)\n",
        "print(\"NB model:\",nb_confu)\n",
        "print(\"My model:\",mm_confu)"
      ],
      "id": "f21bc2a8",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               NB model  My model\n",
            "Accuracy       0.140089  0.954821\n",
            "F1 score       0.140089  0.954821\n",
            "AUC ROC score  0.630883  0.000000\n",
            "NB model: [[[ 6485  3913]\n",
            "  [  407   395]]\n",
            "\n",
            " [[10147   247]\n",
            "  [  685   121]]\n",
            "\n",
            " [[ 8017  2386]\n",
            "  [  471   326]]\n",
            "\n",
            " [[10320    92]\n",
            "  [  783     5]]\n",
            "\n",
            " [[ 9923   468]\n",
            "  [  763    46]]\n",
            "\n",
            " [[10209   186]\n",
            "  [  781    24]]\n",
            "\n",
            " [[ 9327  1062]\n",
            "  [  576   235]]\n",
            "\n",
            " [[10237   166]\n",
            "  [  510   287]]\n",
            "\n",
            " [[10394     1]\n",
            "  [  805     0]]\n",
            "\n",
            " [[10318    72]\n",
            "  [  782    28]]\n",
            "\n",
            " [[10393    30]\n",
            "  [  772     5]]\n",
            "\n",
            " [[10392    10]\n",
            "  [  795     3]]\n",
            "\n",
            " [[ 9651   737]\n",
            "  [  745    67]]\n",
            "\n",
            " [[10156   261]\n",
            "  [  756    27]]]\n",
            "My model: [[[10360    38]\n",
            "  [  105   697]]\n",
            "\n",
            " [[10355    39]\n",
            "  [   16   790]]\n",
            "\n",
            " [[10363    40]\n",
            "  [   92   705]]\n",
            "\n",
            " [[10397    15]\n",
            "  [   10   778]]\n",
            "\n",
            " [[10342    49]\n",
            "  [   16   793]]\n",
            "\n",
            " [[10369    26]\n",
            "  [   10   795]]\n",
            "\n",
            " [[10322    67]\n",
            "  [   31   780]]\n",
            "\n",
            " [[10380    23]\n",
            "  [   21   776]]\n",
            "\n",
            " [[10394     1]\n",
            "  [   42   763]]\n",
            "\n",
            " [[10379    11]\n",
            "  [   65   745]]\n",
            "\n",
            " [[10367    56]\n",
            "  [   13   764]]\n",
            "\n",
            " [[10352    50]\n",
            "  [   10   788]]\n",
            "\n",
            " [[10357    31]\n",
            "  [   25   787]]\n",
            "\n",
            " [[10357    60]\n",
            "  [   50   733]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb2dd830-2700-4fa7-81da-057c621e9c9d"
      },
      "source": [
        "# Part IV: Compare NB to other classification models\n",
        "\n",
        "Now that we've built and validated our NB classifier, we want to evaluate other models on this task.\n",
        "\n",
        "### (5 pts) Task III: Evaluate the perceptron, SVM (linear), and SVM (RBF kernel)\n",
        "Train and evaluate the following models on this dataset, and compare them with the NB models.\n",
        "\n",
        "1. [Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron)\n",
        "2. [Linear-SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)\n",
        "3. [RBF-Kernel-SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"
      ],
      "id": "bb2dd830-2700-4fa7-81da-057c621e9c9d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "852c9e19-7a6b-43d2-86e0-245654cda5dc"
      },
      "source": [
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', Perceptron(tol=1e-3, random_state=0)),\n",
        "])\n",
        "# Fitting our train data to the pipeline\n",
        "text_clf.fit(X_train, Y_train)\n",
        "\n",
        "# Predicting our test data\n",
        "predicted = text_clf.predict(X_test)\n",
        "print(classification_report(X_test,predicted ))"
      ],
      "id": "852c9e19-7a6b-43d2-86e0-245654cda5dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYmOH9VZArtp"
      },
      "source": [
        "param_grid = {'C': np.arange(0.1,1,0.1)}\n",
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', GridSearchCV(LinearSVC(tol=1e-5),param_grid,refit=True,verbose=2)),\n",
        "])\n",
        "# Fitting our train data to the pipeline\n",
        "text_clf.fit(X_train, Y_train)\n",
        "\n",
        "# Predicting our test data\n",
        "predicted = text_clf.predict(Y_test)\n",
        "print(classification_report(Y_test,predicted ))"
      ],
      "id": "RYmOH9VZArtp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ed4dea1-8ce3-4e2d-80e5-7d7ca3a1fc46"
      },
      "source": [
        "### (5 pts) Task IV: Select the best model\n",
        "\n",
        "1. Which model performed the best overall? \n",
        "2. What metric(s) influence this decision?\n",
        "3. Does the model that learns a non-linear decision boundary help?"
      ],
      "id": "9ed4dea1-8ce3-4e2d-80e5-7d7ca3a1fc46"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YynOwwTwBBNh"
      },
      "source": [
        "SVM performs the best.\n",
        "According to the accuracy of test datasets.\n",
        "Yes. it can provide more chocie to train models"
      ],
      "id": "YynOwwTwBBNh"
    }
  ]
}