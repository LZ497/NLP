{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2640818d-560f-4bda-b8c5-dd75ebbe1ec8",
   "metadata": {},
   "source": [
    "# Lab 04: Extracting topics from the NYT comments section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eaeccb-cf1f-47e1-8dfb-06ac139bd64d",
   "metadata": {},
   "source": [
    "# Part I: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984fd692-eeb5-4dfa-b64b-05fd32b5a1c6",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "\n",
    "Data source: https://georgetown.box.com/s/wda8q83elj0khd6khsvdzzyaumpisrg2\n",
    "\n",
    "*Note: This data consists of the comments section from the New York Times in Fall 2020. Some of it's content may be offensive and toxic.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec830e7-b20d-469b-a552-34d6f4f8cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fp = \"../../.local/nyt-comments-part8.csv.zip\"\n",
    "df = pd.read_csv(fp)\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd0a513-4910-4dde-8f11-05a03d6bd5fc",
   "metadata": {},
   "source": [
    "### Dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf76ee4-9892-41ac-94bc-54865add2c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5adbe09-9988-473c-bc02-74827b46b4e5",
   "metadata": {},
   "source": [
    "### Data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b2b00f-b3aa-42bf-8e0f-2ab9be449eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25494be5-0ea2-4284-a714-460efa39efa6",
   "metadata": {},
   "source": [
    "### Example comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac56a2f1-d0b0-4714-8fea-588010a2dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['commentBody'][22]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c93ce6-0986-4b37-b7c6-14caf6002c6e",
   "metadata": {},
   "source": [
    "### Reuse Spacy pipeline from Lab-02 for text normalization & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a87b5-e8d5-4968-b065-bccb27479b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "M = 2500\n",
    "\n",
    "pipeline = spacy.load('en_core_web_sm')\n",
    "\n",
    "# http://emailregex.com/\n",
    "email_re = r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\"\n",
    "\n",
    "# replace = [ (pattern-to-replace, replacement),  ...]\n",
    "replace = [\n",
    "    (r\"<a[^>]*>(.*?)</a>\", r\"\\1\"),  # Matches most URLs\n",
    "    (email_re, \"email\"),            # Matches emails\n",
    "    (r\"(?<=\\d),(?=\\d)\", \"\"),        # Remove commas in numbers\n",
    "    (r\"\\d+\", \"number\"),              # Map digits to special token <numbr>\n",
    "    (r\"[\\t\\n\\r\\*\\.\\@\\,\\-\\/]\", \" \"), # Punctuation and other junk\n",
    "    (r\"\\s+\", \" \")                   # Stips extra whitespace\n",
    "]\n",
    "\n",
    "comments = []\n",
    "for i, comment in enumerate(df['commentBody'][:M]):\n",
    "    for repl in replace:\n",
    "        comment = re.sub(repl[0], repl[1], comment)\n",
    "    comments.append(comment)\n",
    "\n",
    "\n",
    "@Language.component(\"nytComments\")\n",
    "def ng20_preprocess(doc):\n",
    "    tokens = [token for token in doc \n",
    "              if not any((token.is_stop, token.is_punct))]\n",
    "    tokens = [token.lemma_.lower().strip() for token in tokens]\n",
    "    tokens = [token for token in tokens if token]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "pipeline.add_pipe(\"nytComments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c19c8-80ec-4bb2-a7bd-aaad024d4e7b",
   "metadata": {},
   "source": [
    "### Pass data through our Spacy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b68ad3-f1a4-4f2a-b3a5-7f9fc0456ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for comment in comments[:M]:\n",
    "    docs.append(pipeline(comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36b399-6417-4c64-a28f-b9ace7cc4a16",
   "metadata": {},
   "source": [
    "### Compute number of unique words (vocabulary size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f21867e-25ef-4118-8e64-7abc6fb24b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(set(\" \".join(docs).split(\" \")))\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9064a1-c13c-4510-b5be-fc8b6a37c6a6",
   "metadata": {},
   "source": [
    "# Part 2: Build Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d97d061-e99f-4675-9c3a-7fa65350384e",
   "metadata": {},
   "source": [
    "### Build the term-document matrix (i.e., BOW features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95cfcc-ef76-4465-8ff2-d17bd12f9101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_features=vocab_size, max_df=0.95, min_df=0.005, stop_words='english')\n",
    "# tf_vectorizer = TfidfVectorizer(max_features=vocab_size, max_df=0.95, stop_words='english')\n",
    "X = tf_vectorizer.fit_transform(docs)\n",
    "type(X), X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b30a34-698f-46bf-bc50-890a0dd50a1f",
   "metadata": {},
   "source": [
    "### Create a index-to-word map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f86bd6c-8a0a-4c9c-bd51-2896ec3fb5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {idx: word for word, idx in tf_vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5a7d2d-7803-49a9-adf8-e85db8539593",
   "metadata": {},
   "source": [
    "### Check X for correct counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e4417b-79ba-436d-a527-6d5a352e0bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_vectorizer.vocabulary_\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681e038a-c060-44e4-b385-14e402d136ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = tf_vectorizer.vocabulary_['covid']\n",
    "X[0, idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e7176-a6b0-48f1-99ca-2d49cf186862",
   "metadata": {},
   "source": [
    "### Plotting subroutine to visualize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b1c81d-0b21-47f9-8a35-61d435e8dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(K // 5, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[::-1][:n_top_words]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be137c12-6647-40c9-8566-61f3ba0001a1",
   "metadata": {},
   "source": [
    "### (10 pts) Task I: Build a LSA model\n",
    "\n",
    "In this task we are going to build a LSA topic model from scratch. From lecture-04, we learned about LSA from the perspective of document retrieval. For document retrieval, you'll recall that we computed a truncated SVD by choosing some number of dimension $K << N$. This gave us the left singular column vectors, $\\mathbf{U} \\in \\mathbb{R}^{N \\times K}$, and the diagonal singular value matrix, $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{K \\times K}$ that we needed in order to project our queries, $\\mathbf{q} \\in \\mathbb{R}^{N}$, and documents, $\\mathbf{d} \\in \\mathbb{R}^{N}$, into $\\mathbb{R}^{K}$ space. Recall that the operation to do that was:\n",
    "\n",
    "$$\\hat{\\mathbf{q}} = \\mathbf{q}\\mathbf{U}\\mathbf{\\Sigma}^{-1} $$\n",
    "\n",
    "In this task, we're going to evaluate the singular values, $\\sigma_{i,j}$ in $\\mathbf{\\Sigma}$, and their corresponding basis vectors, $\\mathbf{u}^{(j)}$, in $\\mathbf{U}$, to extract the principal themes in the data. Execute the following subtasks.\n",
    "\n",
    "1. For each column vector, print out the top 10 most relevant words.\n",
    "2. Visualize the top 10 words using the `plot_topics()` function provided above.\n",
    "3. What affect does the hyperparameter $K$ have on the result?\n",
    "4. Is there a principled way to determine the best $K$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ef0ca-faf6-4703-ae8e-40b619ec2835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c130a66-7c6e-42c0-b6c2-b7dee5ba993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20 # number of topics (feel free to adjust)\n",
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f673115-7d0e-467b-93ee-7a32dc5c4e15",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (5 pts) Task 3: Compute distances between topics\n",
    "\n",
    "In this task, we're going to compute distances between topics. The right singular vectors produced from SVD are the $K$ most relevant directions in our original $N$-dimensional space. Each topic, or theme, is nothing more than a weighted sum over the words. Compute the cosine distance between each of the topic vectors. Are these distances consistent (in an intuitive sense) with the top words in each topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3ef163-2893-4f43-8c98-291adc5b4bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e3b782-5ced-480d-9abc-b88bc722747a",
   "metadata": {},
   "source": [
    "### (5 pts) Task 2: Perform topic extraction using the NMF and LDA models from sklearn\n",
    "\n",
    "In this task we perform topic extraction using the Non-negative Matrix Factorization (NMF) and Latent Dirichlet Allocation (LDA) models provided by sklearn. \n",
    "\n",
    "1. Fit the NMF and LDA models in the provided cells below.\n",
    "2. Visualize the results using the `plot_topics` function.\n",
    "3. How do the results compare to your home-spun LSA topic model?\n",
    "4. What are the differences between these model that might give rise to these results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d62ac18-e738-47c8-86fc-2279bff226a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17c6a3d-3e8d-48bf-a598-7393aad8789d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f83c20-bd80-4885-a97b-5d192af8de9a",
   "metadata": {},
   "source": [
    "### (5 pts extra credit) Task 3: Fit all three of these topic models on a new dataset\n",
    "\n",
    "Choose a new dataset, perhaps one that naturally breaks down into topics (unlike the comments section of the NYT in Fall 2020), and fit these models using the same approach. Summarize any conclusions that you make with a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06f00c-38a5-4f72-8c9b-5acd7e399974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
